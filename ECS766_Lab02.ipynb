{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLlPtuPgF4jt"
   },
   "source": [
    "# Lab session 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMmueLHXF4jw"
   },
   "source": [
    "## Introduction \n",
    "\n",
    "The aim of this lab (Lab session 2) is for students to gain experience with **Data Preprocessing**, covered in lecture 3, and more specifically with the concepts of **handling missing values**, **handling noisy data**, **data normalisation**, **aggregation**, **sampling**, **discretisation**, and **principal component analysis**. \n",
    "\n",
    "- This lab is the first part of a **two-lab assignment** that covers lectures 3 and 4, which is due on **Sunday, 3 July 2022, 11:59 PM**.\n",
    "- The assignment will account for 10% of your overall grade. Questions in this lab sheet (Lab session 2) will contribute to 5% of your overall grade; questions in the sheet of Lab session 3 will count for another 5% of your overall grade.\n",
    "- <font color = 'maroon'>The **assessed questions** are in the last section of this notebook (Assignment 1, part 1 of 2).</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "- For this two-part assignment you are asked to submit:\n",
    "    - A **report** with the answers to the assessed questions. The report should be in **PDF format** (so **NOT** *doc, docx, notebook* etc). It should contain your name, student number, assignment number (for instance, Assignment 1), module, and marked with question numbers. \n",
    "    - **Completed notebooks**. That is, the two notebooks for the assignment with the code you employed to answer the questions, at the end. That code should be standalone. That is, if someone opens your notebooks, they should be able to scroll down directly to your answers and run the corresponding cells. That means you may have to repeat some import statements, for example.\n",
    "- No other means of submission other than submitting your assignment through the appropriate QM+ link are acceptable at any time. Submissions sent via email will **not** be considered.\n",
    "- Please name your report as follows: Assignment1-StudentName-StudentNumber.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBWaWYotF4jx"
   },
   "source": [
    "## Important notes about the assignment: \n",
    "- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n",
    "- The total assessed coursework is worth 40% of your final grade.\n",
    "- There will be 9 lab sessions and 4 assignments.\n",
    "- One assignment will cover 2 consecutive lab sessions and will be worth 10 marks (percentages of your final grade).\n",
    "- The submission cut-off date will be 7 days after the deadline and penalties will be applied for late submissions in accordance with the School policy on late submissions.\n",
    "- Cases of **Extenuating Circumstances (ECs)** have to go through the proper procedure of the School in due time. Only cases approved by the School in due time can be considered.\n",
    "- For the below assignment questions, you can choose whether to answer them using pen-and-paper or using code. However, please note that questions which explicitly require loading CSV files or using a pre-loaded dataset must be addressed using code. A few assignment questions throughout the module might be easier to complete by hand, although could also be addressed using code. In both cases, either responding to questions manually or using code, please **do make sure to show your work** - i.e., how you derived the result, by showing the code that was used to generate the result that addresses the question and/or by writing down your reasoning or math derivations.\n",
    "- Any mathematics included in the report must be written using Tex typesetting, or a system of comparable quality. A photograph of your handwritten derivations will not be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBJx3HwyF4jy"
   },
   "source": [
    "## 1. Data Quality Issues\n",
    "\n",
    "Poor data quality can have an adverse effect on data mining. Common data quality issues include noise, outliers, missing values, and duplicate data. This section presents examples of Python code to alleviate some of these data quality problems. We begin with an example dataset from the UCI machine learning repository containing information about breast cancer patients. We will first download the dataset using the Pandas read_csv() function, and display its first 5 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A9zZaxiF4jy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data', header=None)\n",
    "data.columns = ['Sample code', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                'Normal Nucleoli', 'Mitoses','Class']\n",
    "\n",
    "data = data.drop(['Sample code'],axis=1)\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d' % (data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShYs5bCPF4jz"
   },
   "source": [
    "### 1.1 Missing Values\n",
    "\n",
    "It is not unusual for an object to be missing one or more attribute values. In some cases, the information was not collected; while in other cases, some attributes are inapplicable to the data instances. This section presents examples of the different approaches for handling missing values. \n",
    "\n",
    "According to the description of the data (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original), the missing values are encoded as '?' in the original data. Our first task is to convert the missing values to NaNs. We can then count the number of missing values in each column of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQ3-kRBQF4j0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = data.replace('?',np.NaN)\n",
    "\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d' % (data.shape[1]))\n",
    "\n",
    "print('Number of missing values:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col,data[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sp4x-fi5F4j1"
   },
   "source": [
    "Observe that only the 'Bare Nuclei' column contains missing values. In the following example, the missing values in the 'Bare Nuclei' column are replaced by the median value of that column. The values before and after replacement are shown for a subset of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IT8oNujeF4j1"
   },
   "outputs": [],
   "source": [
    "data_bn = data['Bare Nuclei']\n",
    "\n",
    "print('Before replacing missing values:')\n",
    "print(data_bn[20:25])\n",
    "data_bn = data_bn.fillna(data_bn.median())\n",
    "\n",
    "print('\\nAfter replacing missing values:')\n",
    "print(data_bn[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lln4hx_UF4j2"
   },
   "source": [
    "Instead of replacing the missing values, another common approach is to discard the data points that contain missing values. This can be easily accomplished by applying the dropna() function to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMF_Bd1GF4j2"
   },
   "outputs": [],
   "source": [
    "print('Number of rows in original data = %d' % (data.shape[0]))\n",
    "\n",
    "data_clean = data.dropna()\n",
    "print('Number of rows after discarding missing values = %d' % (data_clean.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqcnS9GbF4j2"
   },
   "source": [
    "### 1.2 Outliers\n",
    "\n",
    "Outliers are data instances with characteristics that are considerably different from the rest of the dataset. In the example code below, we will draw a boxplot to identify the columns in the table that contain outliers. Note that the values in all columns (except for 'Bare Nuclei') are originally stored as 'int64' whereas the values in the 'Bare Nuclei' column are stored as string objects (since the column initially contains strings such as '?' for representing missing values). Thus, we must  convert the column into numeric values first before creating the boxplot. Otherwise, the column will not be displayed when drawing the boxplot.\n",
    "\n",
    "**Note**: try changing the `figsize` parameter if the plot does not display clearly on your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0b5ePigF4j3"
   },
   "outputs": [],
   "source": [
    "data_bn = data.drop(['Class'],axis=1)\n",
    "data_bn['Bare Nuclei'] = pd.to_numeric(data_bn['Bare Nuclei'])\n",
    "data_bn.boxplot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDrpi2tkF4j3"
   },
   "source": [
    "The boxplots suggest that only 5 of the columns (Marginal Adhesion, Single Epithetial Cell Size, Bland Cromatin, Normal Nucleoli, and Mitoses) contain abnormally high values. To discard the outliers, we can compute the Z-score for each attribute and remove those instances containing attributes with abnormally high or low Z-score (e.g., if Z > 3 or Z <= -3). \n",
    "\n",
    "\n",
    "The following code shows the results of standardizing the columns of the data. Note that missing values (NaN) are not affected by the standardization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4fKcpRS1F4j3"
   },
   "outputs": [],
   "source": [
    "Z = (data_bn-data_bn.mean())/data_bn.std()\n",
    "Z[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yy9zEY0F4j4"
   },
   "source": [
    "The following code shows the results of discarding columns with Z > 3 or Z <= -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cSkvZDYF4j4"
   },
   "outputs": [],
   "source": [
    "print('Number of rows before discarding outliers = %d' % (Z.shape[0]))\n",
    "\n",
    "Z2 = Z.loc[((Z > -3).sum(axis=1)==9) & ((Z <= 3).sum(axis=1)==9),:]\n",
    "print('Number of rows after discarding outlier values = %d' % (Z2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JyC3rJmF4j4"
   },
   "source": [
    "### 1.3 Duplicate Data\n",
    "\n",
    "Some datasets, especially those obtained by merging multiple data sources, may contain duplicates or near duplicate instances. The term deduplication is often used to refer to the process of dealing with duplicate data issues. In the following example, we first check for duplicate instances in the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgRZK1mDF4j4"
   },
   "outputs": [],
   "source": [
    "dups = data.duplicated()\n",
    "print('Number of duplicate rows = %d' % (dups.sum()))\n",
    "data.loc[[11,28]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIBajhP9F4j5"
   },
   "source": [
    "The duplicated() function will return a Boolean array that indicates whether each row is a duplicate of a previous row in the table. The results suggest there are 236 duplicate rows in the breast cancer dataset. For example, the instance with row index 11 has identical attribute values as the instance with row index 28. Although such duplicate rows may correspond to samples for different individuals, in this hypothetical example, we assume that the duplicates are samples taken from the same individual and illustrate below how to remove the duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKU-MXTMF4j5"
   },
   "outputs": [],
   "source": [
    "print('Number of rows before discarding duplicates = %d' % (data.shape[0]))\n",
    "data_deduplicated = data.drop_duplicates()\n",
    "print('Number of rows after discarding duplicates = %d' % (data_deduplicated.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhZBgpeXF4j5"
   },
   "source": [
    "## 2. Aggregation\n",
    "\n",
    "Data aggregation is a preprocessing task where the values of two or more objects are combined into a single object. The motivation for aggregation includes (1) reducing the size of data to be processed, (2) changing the granularity of analysis (from fine-scale to coarser-scale), and (3) improving the stability of the data.\n",
    "\n",
    "In the example below, we will use the daily precipitation time series data for a weather station located at Detroit Metro Airport. The raw data were obtained from the Climate Data Online website (https://www.ncdc.noaa.gov/cdo-web/). The daily precipitation time series will be compared against its monthly values.\n",
    "\n",
    "The code below will load the precipitation time series data and draw a line plot of its daily time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6DIBx7KF4j5"
   },
   "outputs": [],
   "source": [
    "daily = pd.read_csv('DTW_prec.csv', header='infer')\n",
    "daily.index = pd.to_datetime(daily['DATE'])\n",
    "daily = daily['PRCP']\n",
    "ax = daily.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Daily Precipitation (variance = %.4f)' % (daily.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Df9hJHdF4j6"
   },
   "source": [
    "Observe that the daily time series appear to be quite chaotic and varies significantly from one time step to another. The time series can be grouped and aggregated by month to obtain the total monthly precipitation values. The resulting time series appears to vary more smoothly compared to the daily time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SlpupkOF4j6"
   },
   "outputs": [],
   "source": [
    "monthly = daily.groupby(pd.Grouper(freq='M')).sum()\n",
    "ax = monthly.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Monthly Precipitation (variance = %.4f)' % (monthly.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jV6CXS-F4j6"
   },
   "source": [
    "In the example below, the daily precipitation records are grouped and aggregated by year to obtain the annual precipitation values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os26x6XTF4j6"
   },
   "outputs": [],
   "source": [
    "annual = daily.groupby(pd.Grouper(freq='Y')).sum()\n",
    "ax = annual.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Annual Precipitation (variance = %.4f)' % (annual.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P68oQhtGF4j7"
   },
   "source": [
    "## 3. Sampling\n",
    "\n",
    "Sampling is an approach commonly used to facilitate (1) data reduction for exploratory data analysis, (2) scaling up algorithms to big data applications and (3) quantifying uncertainties due to varying data distributions. There are many possible sampling approaches, such as sampling without replacement, where each selected instance is removed from the dataset, and sampling with replacement, where each selected instance is not removed, thus allowing it to be sampled more than once.\n",
    "\n",
    "In the example below, we will apply sampling with replacement and without replacement to the breast cancer dataset obtained from the UCI machine learning repository. We initially display the first five records of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAM4p9S4F4j7"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPl0EipfF4j7"
   },
   "source": [
    "In the following cell, a sample of size 3 is randomly selected (without replacement) from the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B47ATQ99F4j7"
   },
   "outputs": [],
   "source": [
    "sample = data.sample(n=3)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nx_LiuwqF4j7"
   },
   "source": [
    "In the next example, we randomly select 1% of the data (without replacement) and display the selected samples. The random_state argument of the function specifies the seed value of the random number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USrzqmXTF4j8"
   },
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.01, random_state=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR39k-g6F4j8"
   },
   "source": [
    "Finally, we perform a sampling with replacement to create a sample whose size is equal to 1% of the entire data. You should be able to observe duplicate instances in the sample by increasing the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFKvuRtVF4j8"
   },
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.01, replace=True, random_state=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoghbOhJF4j8"
   },
   "source": [
    "## 4. Discretization\n",
    "\n",
    "Discretization is a data preprocessing step that is often used to transform a continuous-valued attribute to a categorical attribute. The example below illustrates two simple but widely-used unsupervised discretization methods (equal width and equal depth -or equal frequency) applied to the 'Clump Thickness' attribute of the breast cancer dataset.\n",
    "\n",
    "First, we plot a histogram that shows the distribution of the attribute values. The value_counts() function can also be applied to count the frequency of each attribute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdLqrRIVF4j8"
   },
   "outputs": [],
   "source": [
    "data['Clump Thickness'].hist(bins=10)\n",
    "data['Clump Thickness'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z30rd_nF4j9"
   },
   "source": [
    "For the equal width method, we can apply the cut() function to discretize the attribute into 4 bins of similar interval widths. The value_counts() function can be used to determine the number of instances in each bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfVXGFr9F4j9"
   },
   "outputs": [],
   "source": [
    "bins = pd.cut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BThohS8tF4j9"
   },
   "source": [
    "For the equal frequency method, the qcut() function can be used to partition the values into 4 bins such that each bin has nearly the same number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxKXDdSTF4j-"
   },
   "outputs": [],
   "source": [
    "bins = pd.qcut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1qmn-n4F4j-"
   },
   "source": [
    "## 5. Principal Component Analysis\n",
    "\n",
    "Principal component analysis (PCA) is a classical method for reducing the number of attributes in the data by projecting the data from its original high-dimensional space onto a lower-dimensional space. The new attributes (also known as principal components) created by PCA have the following properties: (1) they are linear combinations of the original attributes, (2) they are orthogonal (perpendicular) to each other, and (3) they are sorted so that the first ones capture the maximum possible amount of variance in the data.\n",
    "\n",
    "The example below illustrates the application of PCA to an image dataset. There are 16 RGB files, each of which has a size of 111 x 111 pixels. The example code below will read each image file and convert the RGB image into a 111 x 111 x 3 = 36963 feature values. This will create a data matrix of size 16 x 36963."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OEMwx43F4j-"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "numImages = 16\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "imgData = np.zeros(shape=(numImages,36963))\n",
    "\n",
    "for i in range(1,numImages+1):\n",
    "    filename = 'pics/Picture'+str(i)+'.jpg'\n",
    "    img = mpimg.imread(filename)\n",
    "    ax = fig.add_subplot(4,4,i)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    ax.set_title(str(i))\n",
    "    imgData[i-1] = np.array(img.flatten()).reshape(1,img.shape[0]*img.shape[1]*img.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VRhc9rAF4j-"
   },
   "source": [
    "Using PCA, the data points (matrix rows) can be projected onto the subspace spanned by its first two principal directions (the two leading eigenvectors of the covariance matrix). The projected values of the original image data are stored in a pandas DataFrame object named projected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2go-GXYpF4j-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "numComponents = 2\n",
    "pca = PCA(n_components=numComponents)\n",
    "pca.fit(imgData)\n",
    "\n",
    "projected = pca.transform(imgData)\n",
    "projected = pd.DataFrame(projected,columns=['pc1','pc2'],index=range(1,numImages+1))\n",
    "projected['food'] = ['burger', 'burger','burger','burger','drink','drink','drink','drink',\n",
    "                      'pasta', 'pasta', 'pasta', 'pasta', 'chicken', 'chicken', 'chicken', 'chicken']\n",
    "projected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxyjC2GgF4j-"
   },
   "source": [
    "Finally, we draw a scatter plot to display the projected values. Observe that the images of burgers, drinks, and pastas are all projected onto their own clusters. Fried chicken, however, is all over the place and harder to discriminate (shown as black squares in the diagram). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mQNO8nWF4j_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = {'burger':'b', 'drink':'r', 'pasta':'g', 'chicken':'k'}\n",
    "markerTypes = {'burger':'+', 'drink':'x', 'pasta':'o', 'chicken':'s'}\n",
    "\n",
    "for foodType in markerTypes:\n",
    "    d = projected[projected['food']==foodType]\n",
    "    plt.scatter(d['pc1'],d['pc2'],c=colors[foodType],s=60,marker=markerTypes[foodType],label=foodType)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Assignment 1, part 1 of 2</font>\n",
    "\n",
    "Please **show your work** - i.e., show and explain your code/math, and write your reasoning.\n",
    "\n",
    "1. **[1 mark]** Load the CSV file country-income-large.csv, which includes both numerical and categorical attributes. Replace NaN values with the mean of the corresponding variable. Display a scatter plot of the resulting dataset, using the numerical variables only, color-coded according to the \"Region\" column. You are allowed (and encouraged) to seek existing functions for these purposes (e.g. in Pandas).\n",
    "2.**[1 mark]** Apply the following binning techniques on the previously cleaned data, assuming 5 bins in each case:    \n",
    "    1. Equal-frequency binning    \n",
    "    2. Equal-width binning  \n",
    "Report the results.\n",
    "2. **[1 mark]** Compute the Pearson correlation coefficient of the numerical variables. Would you say that the variables are strongly correlated? Did you need to compute the correlation coefficient to reach that conclusion?\n",
    "3. **[1 mark]** Use the Chi-squared test to determine whether the categorical attributes are correlated.\n",
    "1. **[1 mark]** Take the pre-processed breast cancer dataset from subsection 1.1 of this notebook (where we replaced any missing values with their median). Compute the principal components and report the variance explained by each of them (remove the \"Class\" column before doing PCA). Show the scatter plot of all samples along the first two principal components, color-coded according to the \"Class\" column. Ensure that your data is normalized by z-scores prior to performing PCA. Do you think PCA is useful as a preprocessing step for classification in this case?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECS766_Lab02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
